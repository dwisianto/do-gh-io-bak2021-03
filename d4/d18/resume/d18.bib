% Encoding: UTF-8
@misc{mansjur2016reward,
  title={Reward based ranker array for question answer system},
  author={Allen, Corville O and Croutwater, Kyle L and Mansjur, Dwi Sianto},
  year={2016},
  month=feb # "~2",
  publisher={Google Patents},
  note={US Patent 9,251,474},
  url={https://www.google.com/patents/US9251474}
}

@misc{mansjur2014tailoring,
  title={Tailoring Question Answering System Output Based on User Expertise},
  author={Bruno, Nicholas V and Byron, Donna K and Julius Goth, Iii and Mansjur, Dwi Sianto},
  year={2014},
  month=aug # "~14",
  publisher={Google Patents},
  note={US Patent 20160048772A1},
  url={https://www.google.com/patents/US20160048772}
}

@misc{mansjur2014machine,
  title={Machine Learning Model for Level-Based Categorization of Natural Language Parameters},
  author={Bruno, Nicholas V and Byron, Donna K and Julius Goth, Iii and Mansjur, Dwi Sianto},
  year={2014},
  month=sep # "~4",
  publisher={Google Patents},
  note={US Patent 20160071022A1},
  url={https://www.google.tl/patents/US20160071022}
}

@book{mansjur2011statistical,
  title={Statistical pattern recognition approaches for retrieval-based machine translation systems},
  author={Mansjur, Dwi Sianto},
  year={2011},
  publisher={Georgia Institute of Technology}
}

@inproceedings{mansjur2011variability,
  title={Variability regularization in large-margin classification},
  author={Mansjur, Dwi Sianto and Wada, Ted S and Juang, Biing-Hwang},
  booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
  pages={1956--1959},
  year={2011},
  organization={IEEE}
}

@inproceedings{mansjur2008incremental,
  title={Incremental learning of mixture models for simultaneous estimation of class distribution and inter-class decision boundaries},
  author={Mansjur, Dwi Sianto and Juang, Biing Hwang},
  booktitle={Pattern Recognition, 2008. ICPR 2008. 19th International Conference on},
  pages={1--4},
  year={2008},
  organization={IEEE}
}

@INPROCEEDINGS{Mansjur08a,
title={Utilizing non-uniform cost learning for active control of inter-class confusion},
author={Mansjur, D.S. and Qiang Fu and Biing Hwang Juang},
booktitle={Pattern Recognition, 2008. ICPR 2008. 19th International Conference on},
year={2008},
month={Dec.},
volume={},
number={},
pages={1-4},
abstract={In this paper, we demonstrate the use of learning with non-uniform error-cost as a novel technique to design a multiclass cost-sensitive classifier. We investigate two important aspects of the design. First, we show that the learning is effective enough for active control of the multiclass confusion matrix using the cost-matrix. Second, we study the cases when the classifiers have mild model mismatch problems, and conclude that our design still have better performance compared to the conventional cost-sensitive classifier design.},
keywords={learning (artificial intelligence), matrix algebra, pattern classification, probabilityactive control, cost-matrix, inter-class confusion, machine learning technique, mismatch problem, multiclass confusion matrix, multiclass cost-sensitive classifier, nonuniform error-cost learning, probability},
doi={10.1109/ICPR.2008.4761780},
ISSN={1051-4651}, }


@INPROCEEDINGS{Mansjur08b,
title={Incremental learning of mixture models for simultaneous estimation of class distribution and inter-class decision boundaries},
author={Mansjur, D.S. and Juang, B.H.},
booktitle={Pattern Recognition, 2008. ICPR 2008. 19th International Conference on},
year={2008},
month={Dec.},
volume={},
number={},
pages={1-4},
abstract={In this paper, we propose a novel design of high performance Bayes classifier from a small number of observations. The two main challenges to obtain the classifier are the lack of the true functional form of the class-conditional density and the lack of enough data to estimate the parameters of the classifiers. Incremental learning of Gaussian mixture model (GMM) is used to mitigate the lack of the true functional form. Moreover, the classifier uses the training samples from all classes to evaluate the goodness of a particular mixture to be used as the classifier for a specific class. This selection process eases the difficulty of the accurate parameter estimation. Thus, the important trait of the proposed classifier is being able to estimate simultaneously class-conditional density and inter-class boundaries to arbitrary precision. Our experimental results show that the proposed classifier not only has better performance than the conventional classifiers but also requires fewer parameters.},
keywords={Bayes methods, Gaussian processes, learning (artificial intelligence), pattern classificationBayes classifier, Gaussian mixture model, class distribution estimation, incremental learning, interclass decision boundaries},
doi={10.1109/ICPR.2008.4761788},
ISSN={1051-4651}, }


@INPROCEEDINGS{Mansjur08c,
title={Improving Kernel Density Classifier Using Corrective Bandwidth Learning with Smooth Error Loss Function},
author={Mansjur, D.S. and Juang, B.H.},
booktitle={Machine Learning and Applications, 2008. ICMLA '08. Seventh International Conference on},
year={2008},
month={Dec.},
volume={},
number={},
pages={161-167},
abstract={In this paper, we propose a corrective bandwidth learning algorithm for Kernel Density Estimation (KDE)-based classifiers. The objective of the corrective bandwidth learning algorithm is to minimize the expected error-rate. It utilizes a gradient descent technique to obtain the appropriate bandwidths. The proposed classifier is called the "Empirical Mixture Model" (EMM) classifier. Experiments were conducted on a set of multivariate multi-class classification problems with various data sizes. The proposed classifier has an error-rate closer to the true model compared to conventional KDE-based classifiers for both small and large data sizes. Additional experiments on standard machine learning datasets showed that the proposed bandwidth learning algorithm performed very well in general.},
keywords={gradient methods, learning (artificial intelligence), pattern classification corrective bandwidth learning, empirical mixture model classifier, expected error-rate, gradient descent technique, kernel density classifier, kernel density estimation, machine learning, multivariate multiclass classification problems, smooth error loss function},
doi={10.1109/ICMLA.2008.49},
ISSN={}, }


@ARTICLE{Fu08a,
title={Non-Uniform error criteria for automatic pattern and speech recognition},
author={Qiang Fu and Mansjur, D.S. and Juang, B.-H.},
journal={Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on},
year={2008},
month={31 2008-April 4},
volume={},
number={},
pages={1853-1856},
abstract={The classical Bayes decision theory [1] is the foundation of statistical pattern recognition. Conventional applications of the Bayes decision theory result in ubiquitous use of the maximum a posteriori probability (MAP) decision policy and the paradigm of distribution estimation as practice in the design of a statistical pattern recognition system. In this paper, we address the issue of non-uniform error criteria in statistical pattern recognition, and generalize the Bayes decision theory for pattern recognition tasks where errors over different classes have different degrees of significance. We further propose extensions of the method of minimum classification error (MCE) [2] for a practical design of a statistical pattern recognition system to achieve empirical optimality when non-uniform error criteria are prescribed. In addition, we apply our method upon speech recognition tasks. In the context of automatic speech recognition (ASR), we present a variety of training scenarios and weighting strategies under our framework. The experimental demonstrations for both general pattern recognition and continuous speech recognition are provided to support the effectiveness of our new approach.},
keywords={Bayes methods, decision theory, maximum likelihood estimation, speech recognitionMAP decision policy, automatic pattern recognition, automatic speech recognition, classical Bayes decision theory, distribution estimation, maximum a posteriori probability, minimum classification error, nonuniform error criteria, statistical pattern recognition},
doi={10.1109/ICASSP.2008.4517994},
ISSN={1520-6149}, }


@conference {Mansjur11a,
    title = {{Variability regularization in large-margin classification}},
    booktitle = {International Conference on Acoustics, Speech, and Signal Processing},
    year = {2011},
    pages = {1956{\textendash}1959},
    doi = {10.1109/ICASSP.2011.5946892},
    author = {Dwi Sianto Mansjur and Ted S. Wada and Biing-Hwang Juang}
}

@Comment{jabref-meta: databaseType:bibtex;}
